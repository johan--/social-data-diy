{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to dc.js Friendly CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/mnt/home/ubuntu/projects/tools/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:geopy:BeautifulSoup was not found. The SemanticMediaWiki geocoder will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the world...\n",
      "Oh, the world is already out there...\n"
     ]
    }
   ],
   "source": [
    "import sys,re,json,os,csv,glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse\n",
    "import datetime,time,random,traceback\n",
    "from geopy import distance\n",
    "import geolocator\n",
    "geo=geolocator.Geolocator()\n",
    "geo.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220\n",
      "../data/2014-06/DataSift-b44470c030b631466ab6261723158109-1413565092.json\n"
     ]
    }
   ],
   "source": [
    "files=glob.glob('../data/2014-*/*json')\n",
    "files.sort()\n",
    "print len(files)\n",
    "print files[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "documents=[]\n",
    "for file in files:\n",
    "# Cycle through files\n",
    "    fileString=open(file,'r').read().decode('utf-8')\n",
    "    # Read file as one long string and convert to unicode\n",
    "    fileDocuments=[json.loads(line) for line in fileString.split('\\n')]\n",
    "    fileDocuments=[d for d in fileDocuments if d['interaction']['type'] in ['Twitter']]\n",
    "    # Split into lines and load as JSON\n",
    "    documents.extend(fileTweets)\n",
    "    # Add list of tweets from file to global list\n",
    "print len(documents)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "files = None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tweets=[]\n",
    "for document in documents:\n",
    "    fileTweets=[t for t in documents if t['interaction']['tag_tree']['topic'].keys()[0] in ['Discrimination', 'Prevention']]\n",
    "    tweets.extend(fileTweets)\n",
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428706\n"
     ]
    }
   ],
   "source": [
    "tweets=[]\n",
    "for file in files:\n",
    "# Cycle through files\n",
    "    fileString=open(file,'r').read().decode('utf-8')\n",
    "    # Read file as one long string and convert to uniicode\n",
    "    fileDocs=[json.loads(line) for line in fileString.split('\\n')]\n",
    "    fileDocs=[d for d in fileDocs if d['interaction']['tag_tree']['topic'].keys()[0] in ['Discrimination', 'Prevention']]\n",
    "    fileTweets=[t for t in fileDocs if t['interaction']['type'] in ['twitter']]\n",
    "    # Split into lines and load as JSON\n",
    "    tweets.extend(fileTweets)\n",
    "    # Add list of tweets from file to global list\n",
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nTime=0\n",
    "nId=0\n",
    "nCity=0\n",
    "# For counting errors\n",
    "cities=['Belo Horizonte', u'Brasília, Brasilia', u'Cuiabá', 'Curitiba', 'Fortaleza', 'Manaus', 'Natal, Rio Grande do Norte', \n",
    "        'Porto Alegre', 'Recife', 'Rio de Janeiro', 'Salvador, Bahia', u'São Paulo', 'Rio Branco, Acre', u'Maceió', u'Macapá',\n",
    "        u'Vitória, Espírito Santo', u'Goiânia', u'São Luís, Maranhão', 'Campo Grande, Mato Grosso do Sul', u'Belém, Pará',\n",
    "        u'João Pessoa, Paraíba', u'Teresina, Piauí', u'Porto Velho, Rondônia', 'Boa Vista, Roraima', u'Florianópolis',\n",
    "        'Aracaju, Sergipe', 'Palmas, Tocantins']\n",
    "# Define cities to 'snap' coords to\n",
    "coords=[]\n",
    "coords=[geo.geoLocate(c)[0][1:3] for c in cities]\n",
    "# Get coords from geolocator\n",
    "tolerance=120\n",
    "# Set tolerance to snap locations to nearest cities, in KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belo Horizonte (-19.92623, -43.93982)\n",
      "Brasília, Brasilia ('-15.79159', '-47.89558')\n",
      "Cuiabá (-15.41924, -55.89023)\n",
      "Curitiba (-25.50395, -49.29082)\n",
      "Fortaleza (-3.72271, -38.52465)\n",
      "Manaus (-3.04361, -60.01282)\n",
      "Natal, Rio Grande do Norte ('-5.795', '-35.20944')\n",
      "Porto Alegre (-30.11462, -51.16393)\n",
      "Recife (-8.01175, -34.95291)\n",
      "Rio de Janeiro (-22.0, -42.5)\n",
      "Salvador, Bahia (-12.97177, -38.50811)\n",
      "São Paulo (-22.0, -49.0)\n",
      "Rio Branco, Acre ('-9.97472', '-67.81')\n",
      "Maceió (-9.66583, -35.73528)\n",
      "Macapá (0.59873, -50.76849)\n",
      "Vitória, Espírito Santo ('-20.29048', '-40.28808')\n",
      "Goiânia (-16.64019, -49.25993)\n",
      "São Luís, Maranhão ('-2.64949', '-44.30441')\n",
      "Campo Grande, Mato Grosso do Sul ('-20.44278', '-54.64639')\n",
      "Belém, Pará ('-1.34341', '-48.41816')\n",
      "João Pessoa, Paraíba ('-7.17088', '-34.86536')\n",
      "Teresina, Piauí ('-5.08917', '-42.80194')\n",
      "Porto Velho, Rondônia ('-8.76194', '-63.90389')\n",
      "Boa Vista, Roraima ('2.81972', '-60.67333')\n",
      "Florianópolis (-27.61455, -48.50116)\n",
      "Aracaju, Sergipe ('-10.91111', '-37.07167')\n",
      "Palmas, Tocantins ('-10.21278', '-48.36028')\n"
     ]
    }
   ],
   "source": [
    "outFile=csv.writer(open('cities.csv','w'),delimiter='\\t')\n",
    "for i,j in zip(cities,coords):\n",
    "    print i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'twitter': {u'lang': u'pt', u'source': u'<a href=\"https://twitter.com/download/android\" rel=\"nofollow\">Twitter for  Android</a>', u'text': u'Nao curto ropa apertada isso e coisa de boiola,.so mais minha peita.gigante, meu bone e meu board debaixo do pe', u'created_at': u'Thu, 19 Jun 2014 06:06:06 +0000', u'filter_level': u'medium', u'user': {u'lang': u'pt', u'created_at': u'Thu, 07 Jun 2012 05:29:41 +0000', u'utc_offset': -10800, u'id_str': u'601583939', u'statuses_count': 39938, u'name': u'Fabio Andrey  \\u270c', u'friends_count': 511, u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/468547366929567744/GmukbbI6_normal.jpeg', u'time_zone': u'Brasilia', u'profile_image_url': u'http://pbs.twimg.com/profile_images/468547366929567744/GmukbbI6_normal.jpeg', u'followers_count': 788, u'screen_name': u'Sou_Nigga', u'location': u'sao leo', u'favourites_count': 5178, u'verified': False, u'geo_enabled': True, u'listed_count': 1, u'id': 601583939, u'description': u'Bibi'}, u'id': u'479505381132345344'}, u'interaction': {u'author': {u'username': u'Sou_Nigga', u'name': u'Fabio Andrey  \\u270c', u'language': u'pt', u'link': u'http://twitter.com/Sou_Nigga', u'avatar': u'http://pbs.twimg.com/profile_images/468547366929567744/GmukbbI6_normal.jpeg', u'id': 601583939}, u'created_at': u'Thu, 19 Jun 2014 06:06:06 +0000', u'tag_tree': {u'topic': {u'Discrimination': [u'Negative']}}, u'content': u'Nao curto ropa apertada isso e coisa de boiola,.so mais minha peita.gigante, meu bone e meu board debaixo do pe', u'source': u'Twitter for  Android', u'link': u'http://twitter.com/Sou_Nigga/status/479505381132345344', u'received_at': 1403157966.5953, u'type': u'twitter', u'id': u'1e3f777ccc6dab00e074c042563137a0', u'schema': {u'version': 3}}, u'demographic': {u'gender': u'male'}, u'language': {u'confidence': 99, u'tag': u'pt', u'tag_extended': u'pt'}, u'klout': {u'score': 44}}\n"
     ]
    }
   ],
   "source": [
    "print tweets[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getClosestCity(tCoords):\n",
    "  '''Takes tuple of coordinates, cycles through cities \n",
    "     in global variable <cities>, reads their coords from\n",
    "     global variable <coords> and returns closest\n",
    "     ------\n",
    "     returns tuple of coords of closest city,city name\n",
    "     OR None, if no city within tolerance'''\n",
    "  dist=999999\n",
    "  closest='ZZZZ'\n",
    "  cCoords=[]\n",
    "  for c,cc in enumerate(cities):\n",
    "    cDist=distance.distance(tCoords,coords[c])\n",
    "    if cDist<dist:\n",
    "      dist=cDist\n",
    "      closest=cc\n",
    "      cCoords=coords[c]\n",
    "  if dist<tolerance:\n",
    "    return cCoords,closest\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'VALENTINE': {'gender': 'mm',\n",
       "  'probability': 0.7840717162530856,\n",
       "  'volume_female': 1662.0,\n",
       "  'volume_male': 6035.0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gender\n",
    "g=gender.Gender()\n",
    "g.gender(tweets[1]['interaction']['author']['name'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mungeDate(dummyTime):\n",
    "  '''Takes Twitter timestamp\n",
    "     ------\n",
    "     returns iso format timestamp -> YYY-MM-DD hh:mm:ss\n",
    "  '''\n",
    "  # Get from this format: Thu, 02 Jan 2014 16:26:15 +0000...\n",
    "  timeStruct=time.strptime(dummyTime,'%a, %d %b %Y %H:%M:%S +0000')\n",
    "  # Gets list with date/time components\n",
    "  return str(timeStruct[0])+'-'+str(timeStruct[1])+'-'+str(timeStruct[2])+' '+str(timeStruct[3])+':'+str(timeStruct[4])+':'+str(timeStruct[5])\n",
    "  # ...into this format mm/DD/YYYYYYY-MM-DD hh:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mungeDate(dummyTime):\n",
    "  '''Takes Twitter timestamp\n",
    "     ------\n",
    "     returns iso format timestamp -> YYY-MM-DD hh:mm:ss\n",
    "  '''\n",
    "  # Get from this format: Thu, 02 Jan 2014 16:26:15 +0000...\n",
    "  timeStruct=datetime.datetime.strptime(dummyTime,'%a, %d %b %Y %H:%M:%S +0000')\n",
    "  # Gets list with date/time components\n",
    "  return timeStruct\n",
    "  # ...into this format mm/DD/YYYYYYY-MM-DD hh:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-19.92623, -43.93982), ('-15.79159', '-47.89558'), (-15.41924, -55.89023), (-25.50395, -49.29082), (-3.72271, -38.52465), (-3.04361, -60.01282), ('-5.795', '-35.20944'), (-30.11462, -51.16393), (-8.01175, -34.95291), (-22.0, -42.5), (-12.97177, -38.50811), (-22.0, -49.0), ('-9.97472', '-67.81'), (-9.66583, -35.73528), (0.59873, -50.76849), ('-20.29048', '-40.28808'), (-16.64019, -49.25993), ('-2.64949', '-44.30441'), ('-20.44278', '-54.64639'), ('-1.34341', '-48.41816'), ('-7.17088', '-34.86536'), ('-5.08917', '-42.80194'), ('-8.76194', '-63.90389'), ('2.81972', '-60.67333'), (-27.61455, -48.50116), ('-10.91111', '-37.07167'), ('-10.21278', '-48.36028')]\n",
      "(-22.0, -49.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((-22.0, -49.0), u'S\\xe3o Paulo')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print coords\n",
    "print coords[cities.index(u'São Paulo')]\n",
    "getClosestCity(coords[cities.index(u'São Paulo')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 405176 14140 0 0 0\n"
     ]
    }
   ],
   "source": [
    "outFile=csv.writer(open('../data/all.csv','w'))\n",
    "# Open output file\n",
    "nTime=nId=nCity=nRange=nCategory=nSubCategory=nTopic=0\n",
    "# Reset error counters\n",
    "\n",
    "outFile.writerow(['city','lat','lon','origdate','topic']) \n",
    "\n",
    "for t,tweet in enumerate(tweets):\n",
    "  cityCoords=None\n",
    "  try:\n",
    "    tTime=tweet['interaction']['created_at']\n",
    "  except:\n",
    "    nTime+=1\n",
    "  try:\n",
    "    id=tweet['interaction']['id']\n",
    "  except:\n",
    "    nId+=1\n",
    "  try:\n",
    "    category=tweet['interaction']['tag_tree']['topic'].keys()[0]\n",
    "  except:\n",
    "    nCategory+=1\n",
    "  try:\n",
    "    subCategory=tweet['interaction']['tag_tree']['topic'].values()[0][0]\n",
    "  except:\n",
    "    nSubCategory+=1\n",
    "  try:\n",
    "    topic = category + \"_\" + subCategory\n",
    "  except:\n",
    "    nTopic+=1\n",
    "  if 'geo' in tweet['twitter'].keys():\n",
    "    res=getClosestCity([tweet['twitter']['geo']['latitude'],tweet['twitter']['geo']['longitude']])   \n",
    "    if res:\n",
    "    # If location doesn't snap to chosen cities, within tolerance, then throw away\n",
    "      (cityCoords,city)=res\n",
    "      outFile.writerow([city.partition(',')[0].encode(\"utf-8\"),cityCoords[0],cityCoords[1],mungeDate(tTime),topic])\n",
    "    else:\n",
    "      nRange+=1\n",
    "  else:\n",
    "    nCity+=1\n",
    "    # print tweet\n",
    "    # print 'FAILING...'\n",
    "    # print tweet.keys()\n",
    "    # sys.exit(1)\n",
    "    # All these tweets should have lat/long, if not stop and find out why\n",
    "print nTime,nId,nCity,nRange,nCategory,nSubCategory,nTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city,lat,lon,origdate,topic\n",
      "Porto Alegre,-30.11462,-51.16393,2014-06-19 06:01:11,Prevention_Positive\n",
      "Fortaleza,-3.72271,-38.52465,2014-06-19 09:06:28,Discrimination_Negative\n",
      "Recife,-8.01175,-34.95291,2014-06-19 00:22:09,Discrimination_Negative\n",
      "Brasília,-15.79159,-47.89558,2014-06-19 02:07:21,Discrimination_Negative\n",
      "Fortaleza,-3.72271,-38.52465,2014-06-19 23:55:34,Discrimination_Negative\n",
      "Salvador,-12.97177,-38.50811,2014-06-19 00:02:22,Prevention_Positive\n",
      "Rio de Janeiro,-22.0,-42.5,2014-06-19 00:27:53,Discrimination_Negative\n",
      "Porto Alegre,-30.11462,-51.16393,2014-06-19 02:33:42,Discrimination_Negative\n",
      "Belo Horizonte,-19.92623,-43.93982,2014-06-19 03:19:31,Discrimination_Negative\n",
      "  9371  24351 700416 ../data/all.csv\n"
     ]
    }
   ],
   "source": [
    "!head ../data/all.csv\n",
    "!wc ../data/all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url(https://fonts.googleapis.com/css?family=Roboto+Slab:400,100);\n",
       "@import url(https://fonts.googleapis.com/css?family=Roboto+Mono:300);\n",
       "\n",
       "body{\n",
       "  font-family: \"Helvetica\", \"Helvetica Neue\", sans-serif;\n",
       "}\n",
       "\n",
       "#notebook {\n",
       "  font-size: 1.1em !important;\n",
       "  font-weight: 100;\n",
       "}\n",
       "\n",
       "a {\n",
       "\tcolor: #138331;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5 {\n",
       "  font-family: \"Roboto Slab\", sans-serif;\n",
       "  font-weight: 100 !important;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "  font-family: \"Roboto Mono\", monospace;\n",
       "  font-weight: 300;\n",
       "}\n",
       "\n",
       "div.prompt{\n",
       "  font-family: \"Roboto Mono\", monospace;\n",
       "  font-weight: 300;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "styles = open(\"../css/custom.css\", \"r\").read()\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
