{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import retrieved JSON files (from S3)\n",
    "2. Read in individual tweets\n",
    "3. Geolocate\n",
    "4. Add gender\n",
    "5. Create CSV file (and drop unwanted data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data and Enrich It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/home/ubuntu/projects/tools/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys,re,json,os,csv,glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse\n",
    "import time,random,traceback\n",
    "import cPickle as pickle\n",
    "#from geopy import distance\n",
    "#import geolocator\n",
    "#geo=geolocator.Geolocator()\n",
    "#geo.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Pickle File with Tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "picklepath = '../data/raw/tweets.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pickle.load( open(picklepath, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 18 tweets in total\n"
     ]
    }
   ],
   "source": [
    "print('We have %d tweets in total' % len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Rid of Line Breaks in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed removing line breaks in 0 tweets\n"
     ]
    }
   ],
   "source": [
    "tweetLinebreakError=0\n",
    "\n",
    "for tweet in tweets:\n",
    "  try:\n",
    "    tweet['text'] = tweet['text'].replace('\\n', ' ').replace('\\r', '')\n",
    "  except:\n",
    "    tweetLinebreakError+=1\n",
    "    tweet['text'] = 'NaN'\n",
    "\n",
    "print('Failed removing line breaks in %d tweets' % tweetLinebreakError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocate From User Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't geolocate 263646 tweets\n",
      "Geolocated 198249 tweets\n",
      "Managed to geolocate 42 percent\n"
     ]
    }
   ],
   "source": [
    "geoError=0\n",
    "for tweet in tweets:\n",
    "  try:\n",
    "    tweet['geolocated']=geo.geoLocate(tweet['twitter']['retweet']['user']['location'])[0][3] \n",
    "  except:\n",
    "    try:\n",
    "        tweet['geolocated']=geo.geoLocate(tweet['twitter']['user']['location'])[0][3]\n",
    "    except:\n",
    "        geoError+=1\n",
    "        tweet['geolocated']=None\n",
    "print('Couldn\\'t geolocate %d tweets' % geoError)\n",
    "print('Geolocated %d tweets' % (len(tweets) - (geoError)))\n",
    "print('Managed to geolocate %d percent' % (100.0*(1.0-(float(geoError)/len(tweets)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'CH'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing that it worked\n",
    "tweets[0]['geolocated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'VALENTINE': {'gender': 'mm',\n",
       "  'probability': 0.7840717162530856,\n",
       "  'volume_female': 3324.0,\n",
       "  'volume_male': 12070.0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gender\n",
    "g=gender.Gender()\n",
    "g.gender(tweets[1]['interaction']['author']['name']) #Testing that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't add gender probability for 21030 tweets\n",
      "Managed to add gender to 95 p.c.\n"
     ]
    }
   ],
   "source": [
    "# Gender of tweeter or retweeter\n",
    "genderError=0\n",
    "for tweet in tweets:\n",
    "  try:\n",
    "    tweet['gender']=g.gender(tweet['interaction']['author']['name'])\n",
    "  except:\n",
    "    genderError+=1\n",
    "    tweet['gender']=None\n",
    "print('Couldn\\'t add gender probability for %d tweets' % genderError)\n",
    "print('Managed to add gender to %d p.c.' % (100.0*(1.0-(float(genderError)/len(tweets)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mm'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing that it worked\n",
    "tweets[1]['gender'].values()[0]['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Topics and Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics=['Campaign','Discrimination','Prevention','Testing']\n",
    "emptyTopics=[0 for t in topics]\n",
    "header=['id','time','content','type','datasift_lang','twitter_lang','twitter_location','UNGP_location',\n",
    "        'datasift_gender','UNGP_gender','gender_prob','followers','friends','topic','subtopic']\n",
    "header.extend(topics)\n",
    "header.extend(['interaction_hashtags','twitter_mentions','normalised_links','links_domain','user_description',\n",
    "               'user_screen_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/all.json','wb') as f: f.write(json.dumps(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outFile=csv.writer(open('../data/all.tsv','wb'),delimiter='\\t')\n",
    "outFile.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ID errors.\n",
      "0 Date errors.\n",
      "0 Content errors.\n",
      "0 Type errors.\n",
      "9112 DataSift language errors.\n",
      "192371 Twitter language errors.\n",
      "295712 Twitter Location errors.\n",
      "263646 UNGP Location errors.\n",
      "187428 Gender errors.\n",
      "205330 UNGP gender errors.\n",
      "205330 UNGP gender probability errors.\n",
      "192371 Follower errors.\n",
      "192371 Friends errors.\n",
      "0 Topic Key errors.\n",
      "0 Topic Value errors.\n",
      "0 Topic errors.\n",
      "0 Topic lengtherrors.\n",
      "404584 Interaction hashtag errors.\n",
      "367573 Interaction mention errors.\n",
      "404903 Interaction link errors.\n",
      "414985 Domain errors.\n",
      "227110 Description errors.\n",
      "192371 Screen name errors.\n"
     ]
    }
   ],
   "source": [
    "nIdError=0\n",
    "nDateError=0\n",
    "nContentError=0\n",
    "nTypeError=0\n",
    "nLanguageError=0\n",
    "nTwitterLanguageError=0\n",
    "nLocationError=0\n",
    "nUngpLocationError=0\n",
    "nGenderError=0\n",
    "nUngpGenderError=0\n",
    "nUngpGenderProbError=0\n",
    "nFollowersError=0\n",
    "nFriendsError=0\n",
    "nTopicKeyError=0\n",
    "nTopicValueError=0\n",
    "nTopicError=0\n",
    "nTopicLengthError=0\n",
    "nTagsError=0\n",
    "nMentionsError=0\n",
    "nLinksError=0\n",
    "nDomainsError=0\n",
    "nDescriptionError=0\n",
    "nScreenNameError=0\n",
    "\n",
    "documents=[]\n",
    "\n",
    "for tweet in tweets:\n",
    "  outList=[]\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['id'])\n",
    "    documents.append(tweet['interaction']['id'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nIdError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['created_at'])\n",
    "    documents.append(tweet['interaction']['created_at'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nDateError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['content'].encode('utf-8').replace('\\n',' '))\n",
    "    documents.append(tweet['interaction']['content'].encode('utf-8').replace('\\n',' '))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nContentError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['type'].encode('utf-8'))\n",
    "    documents.append(tweet['interaction']['type'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nTypeError+=1\n",
    "  try:\n",
    "    outList.append(tweet['language']['tag'].encode('utf-8'))\n",
    "    documents.append(tweet['language']['tag'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nLanguageError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['lang'].encode('utf-8'))\n",
    "    documents.append(tweet['twitter']['lang'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nTwitterLanguageError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['user']['location'].encode('utf-8'))\n",
    "    documents.append(tweet['twitter']['user']['location'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nLocationError+=1\n",
    "  try:\n",
    "    outList.append(tweet['geolocated'].encode('utf-8'))\n",
    "    documents.append(tweet['geolocated'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nUngpLocationError+=1\n",
    "  try:\n",
    "    outList.append(tweet['demographic']['gender'].encode('utf-8'))\n",
    "    documents.append(tweet['demographic']['gender'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nGenderError+=1\n",
    "  try:\n",
    "    outList.append(tweet['gender'].values()[0]['gender'].encode('utf-8'))\n",
    "    documents.append(tweet['gender'].values()[0]['gender'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nUngpGenderError+=1\n",
    "  try:\n",
    "    outList.append(tweet['gender'].values()[0]['probability'])\n",
    "    documents.append(tweet['gender'].values()[0]['probability'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nUngpGenderProbError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['user']['followers_count'])\n",
    "    documents.append(tweet['twitter']['user']['followers_count'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nFollowersError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['user']['friends_count'])\n",
    "    documents.append(tweet['twitter']['user']['friends_count'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nFriendsError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['tag_tree']['topic'].keys()[0].encode('utf-8'))\n",
    "    documents.append(tweet['interaction']['tag_tree']['topic'].keys()[0].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nTopicKeyError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['tag_tree']['topic'].values()[0][0].encode('utf-8'))\n",
    "    documents.append(tweet['interaction']['tag_tree']['topic'].values()[0][0].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    nTopicValueError+=1\n",
    "  try:\n",
    "    tweetTopics=tweet['interaction']['tag_tree']['topic']\n",
    "    binaryTopics=[0 for e in emptyTopics]\n",
    "    for t in tweetTopics:\n",
    "        binaryTopics[topics.index(t)]=1\n",
    "    outList.extend(binaryTopics)\n",
    "    documents.extend(binaryTopics)\n",
    "  except:\n",
    "    outList.extend(emptyTopics)\n",
    "    nTopicError+=1\n",
    "  try:\n",
    "    tweetTags=','.join([h.lower() for h in tweet['interaction']['hashtags']])\n",
    "    outList.append(tweetTags.decode('utf-8'))\n",
    "    documents.append(tweetTags.decode('utf-8'))\n",
    "  except:\n",
    "    nTagsError+=1\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "  try:\n",
    "    tweetMentions=','.join([m.lower() for m in tweet['twitter']['mentions']])\n",
    "    outList.append(tweetMentions.decode('utf-8'))\n",
    "    documents.append(tweetMentions.decode('utf-8'))\n",
    "  except:\n",
    "    nMentionsError+=1\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "  try:\n",
    "    tweetLinks=','.join(tweet['links']['normalized_url'])\n",
    "    outList.append(tweetLinks.decode('utf-8'))\n",
    "    documents.append(tweetLinks.decode('utf-8'))\n",
    "  except:\n",
    "    nLinksError+=1\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "  try:\n",
    "    tweetDomain=','.join(tweet['links']['domain'])\n",
    "    outList.append(tweetDomain.decode('utf-8'))\n",
    "    documents.append(tweetDomain.decode('utf-8'))\n",
    "  except:\n",
    "    nDomainsError+=1\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['user']['description'].encode('utf-8'))\n",
    "    documents.append(tweet['twitter']['user']['description'].encode('utf-8'))\n",
    "  except:\n",
    "    nDescriptionError+=1\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['user']['screen_name'].encode('utf-8'))\n",
    "    documents.append(tweet['twitter']['user']['screen_name'].encode('utf-8'))\n",
    "  except:\n",
    "    nScreenNameError+=1\n",
    "    outList.append('NaN')\n",
    "    documents.append('NaN')\n",
    "    \n",
    "            \n",
    "  outFile.writerow(outList)\n",
    "\n",
    "print \"%d ID errors.\" % nIdError\n",
    "print \"%d Date errors.\" % nDateError\n",
    "print \"%d Content errors.\" % nContentError\n",
    "print \"%d Type errors.\" % nTypeError\n",
    "print \"%d DataSift language errors.\" % nLanguageError\n",
    "print \"%d Twitter language errors.\" % nTwitterLanguageError\n",
    "print \"%d Twitter Location errors.\" % nLocationError\n",
    "print \"%d UNGP Location errors.\" % nUngpLocationError\n",
    "print \"%d Gender errors.\" % nGenderError\n",
    "print \"%d UNGP gender errors.\" % nUngpGenderError\n",
    "print \"%d UNGP gender probability errors.\" % nUngpGenderProbError\n",
    "print \"%d Follower errors.\" % nFollowersError\n",
    "print \"%d Friends errors.\" % nFriendsError\n",
    "print \"%d Topic Key errors.\" % nTopicKeyError\n",
    "print \"%d Topic Value errors.\" % nTopicValueError\n",
    "print \"%d Topic errors.\" % nTopicError\n",
    "print \"%d Topic lengtherrors.\" % nTopicLengthError\n",
    "print \"%d Interaction hashtag errors.\" % nTagsError\n",
    "print \"%d Interaction mention errors.\" % nMentionsError\n",
    "print \"%d Interaction link errors.\" % nLinksError\n",
    "print \"%d Domain errors.\" % nDomainsError\n",
    "print \"%d Description errors.\" % nDescriptionError\n",
    "print \"%d Screen name errors.\" % nScreenNameError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url(https://fonts.googleapis.com/css?family=Roboto+Slab:400,100);\n",
       "@import url(https://fonts.googleapis.com/css?family=Roboto+Mono:300);\n",
       "\n",
       "body{\n",
       "  font-family: \"Helvetica\", \"Helvetica Neue\", sans-serif;\n",
       "}\n",
       "\n",
       "#notebook {\n",
       "  font-size: 1.1em !important;\n",
       "  font-weight: 100;\n",
       "}\n",
       "\n",
       "a {\n",
       "\tcolor: #138331;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5 {\n",
       "  font-family: \"Roboto Slab\", sans-serif;\n",
       "  font-weight: 100 !important;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "  font-family: \"Roboto Mono\", monospace;\n",
       "  font-weight: 300;\n",
       "}\n",
       "\n",
       "div.prompt{\n",
       "  font-family: \"Roboto Mono\", monospace;\n",
       "  font-weight: 300;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "styles = open(\"../css/custom.css\", \"r\").read()\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
