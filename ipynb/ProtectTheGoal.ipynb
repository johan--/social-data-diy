{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import retrieved JSON files (from S3)\n",
    "2. Read in individual tweets\n",
    "3. Geolocate\n",
    "4. Add gender\n",
    "5. Create CSV file (and drop unwanted data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data and Enrich It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/mnt/home/ubuntu/projects/tools/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:geopy:BeautifulSoup was not found. The SemanticMediaWiki geocoder will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the world...\n",
      "Oh, the world is already out there...\n"
     ]
    }
   ],
   "source": [
    "import sys,re,json,os,csv,glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse\n",
    "import time,random,traceback\n",
    "from geopy import distance\n",
    "import geolocator\n",
    "geo=geolocator.Geolocator()\n",
    "geo.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create data folder\n",
    "# !mkdir -p ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab (JSON) files from S3, takes a bunch of time\n",
    "# s3cmd sync s3://plny-protectthegoal/tweets ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/2014-06/DataSift-5feb4f2f6ae45ec94ee52ef01eb40052-1405010687.json\n"
     ]
    }
   ],
   "source": [
    "files=glob.glob('../data/2014-06/DataSift*json')\n",
    "files.sort()\n",
    "print files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 69 files\n"
     ]
    }
   ],
   "source": [
    "# Find Number of JSON Files\n",
    "print('We have a total of %d files' % len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 85 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets=[]\n",
    "for file in files:\n",
    "# Cycle through files\n",
    "    fileString=open(file,'r').read().decode('utf-8')\n",
    "    # Read file as one long string and convert to unicode\n",
    "    fileTweets=[json.loads(line) for line in fileString.split('\\n')]\n",
    "    # Split into lines and load as JSON\n",
    "    tweets.extend(fileTweets)\n",
    "    # Add list of tweets from file to global list\n",
    "print('We have %d tweets' % len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocate From User Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't geolocate 10 tweets\n",
      "Managed to geolocate 88 p.c.\n"
     ]
    }
   ],
   "source": [
    "geoError=0\n",
    "for tweet in tweets:\n",
    "  try:\n",
    "    tweet['geolocated']=geo.geoLocate(tweet['twitter']['retweet']['user']['location'])[0][3] \n",
    "  except:\n",
    "    try:\n",
    "        tweet['geolocated']=geo.geoLocate(tweet['twitter']['user']['location'])[0][3]\n",
    "    except:\n",
    "        geoError+=1\n",
    "        tweet['geolocated']=None\n",
    "print('Couldn\\'t geolocate %d tweets' % geoError)\n",
    "print('Managed to geolocate %d p.c.' % (100.0*(1.0-(float(geoError)/len(tweets)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing that it worked\n",
    "tweets[0]['geolocated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'MIGUEL LUCERO': {'gender': 'male',\n",
       "  'probability': 0.9940737157197714,\n",
       "  'volume_female': 980.0,\n",
       "  'volume_male': 164385.0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gender\n",
    "g=gender.Gender()\n",
    "g.gender(tweets[2]['interaction']['author']['name']) #Testing that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't add gender probability for 0 tweets\n",
      "Managed to add gender to 100 p.c.\n"
     ]
    }
   ],
   "source": [
    "# Gender of tweeter or retweeter\n",
    "genderError=0\n",
    "for tweet in tweets:\n",
    "  try:\n",
    "    tweet['gender']=g.gender(tweet['interaction']['author']['name'])\n",
    "  except:\n",
    "    genderError+=1\n",
    "    tweet['gender']=None\n",
    "print('Couldn\\'t add gender probability for %d tweets' % genderError)\n",
    "print('Managed to add gender to %d p.c.' % (100.0*(1.0-(float(genderError)/len(tweets)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-894e52995262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing that it worked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Testing that it worked\n",
    "tweets[0]['gender'].values()[0]['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/protectthegoal.json','wb') as f: f.write(json.dumps(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outFile=csv.writer(open('../data/protectthegoal.csv','wb'),delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ID errors.\n",
      "0 Date errors.\n",
      "0 Content errors.\n",
      "0 Type errors.\n",
      "2 DataSift language errors.\n",
      "0 Twitter language errors.\n",
      "2 Twitter Location errors.\n",
      "0 Twitter Latitude errors.\n",
      "0 Twitter Longitude errors.\n",
      "10 UNGP Location errors.\n",
      "29 Gender errors.\n",
      "23 UNGP gender errors.\n",
      "23 UNGP gender probability errors.\n",
      "0 Follower errors.\n",
      "0 Friends errors.\n",
      "85 Sentiment errors.\n",
      "0 Topic errors.\n",
      "0 Sub-topic errors.\n"
     ]
    }
   ],
   "source": [
    "nIdError=0\n",
    "nDateError=0\n",
    "nContentError=0\n",
    "nTypeError=0\n",
    "nLanguageError=0\n",
    "nTwitterLanguageError=0\n",
    "nLocationError=0\n",
    "nLatError=0\n",
    "nLongError=0\n",
    "nUngpLocationError=0\n",
    "nGenderError=0\n",
    "nUngpGenderError=0\n",
    "nUngpGenderProbError=0\n",
    "nFollowersError=0\n",
    "nFriendsError=0\n",
    "nSentimentError=0\n",
    "nTopicError=0\n",
    "nSubTopicError=0\n",
    "\n",
    "documents=[]\n",
    "\n",
    "for tweet in tweets:\n",
    "  outList=[]\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['id'])\n",
    "    documents.append(tweet['interaction']['id'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nIdError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['created_at'])\n",
    "    documents.append(tweet['interaction']['created_at'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nDateError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['content'].encode('utf-8'))\n",
    "    documents.append(tweet['interaction']['content'].encode('utf-8'))\n",
    "  except:\n",
    "    #print traceback.print_exc()\n",
    "    #print tweet['interaction']['content']\n",
    "    outList.append('NaN')\n",
    "    nContentError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['type'].encode('utf-8'))\n",
    "    documents.append(tweet['interaction']['type'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nTypeError+=1\n",
    "  try:\n",
    "    outList.append(tweet['language']['tag'].encode('utf-8'))\n",
    "    documents.append(tweet['language']['tag'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nLanguageError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['lang'].encode('utf-8'))\n",
    "    documents.append(tweet['twitter']['lang'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nTwitterLanguageError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['user']['location'].encode('utf-8'))\n",
    "    documents.append(tweet['twitter']['user']['location'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nLocationError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['geo']['latitude'])\n",
    "    documents.append(tweet['twitter']['geo']['latitude'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nLatError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['geo']['longitude'])\n",
    "    documents.append(tweet['twitter']['geo']['longitude'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nLongError+=1\n",
    "  try:\n",
    "    outList.append(tweet['geolocated'].encode('utf-8'))\n",
    "    documents.append(tweet['geolocated'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nUngpLocationError+=1\n",
    "  try:\n",
    "    outList.append(tweet['demographic']['gender'].encode('utf-8'))\n",
    "    documents.append(tweet['demographic']['gender'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nGenderError+=1\n",
    "  try:\n",
    "    outList.append(tweet['gender'].values()[0]['gender'].encode('utf-8'))\n",
    "    documents.append(tweet['gender'].values()[0]['gender'].encode('utf-8'))\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nUngpGenderError+=1\n",
    "  try:\n",
    "    outList.append(tweet['gender'].values()[0]['probability'])\n",
    "    documents.append(tweet['gender'].values()[0]['probability'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nUngpGenderProbError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['user']['followers_count'])\n",
    "    documents.append(tweet['twitter']['user']['followers_count'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nFollowersError+=1\n",
    "  try:\n",
    "    outList.append(tweet['twitter']['user']['friends_count'])\n",
    "    documents.append(tweet['twitter']['user']['friends_count'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nFriendsError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['tag_tree']['sentiment'].values()[0])\n",
    "    documents.append(tweet['interaction']['tag_tree']['sentiment'].values()[0])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nSentimentError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['tag_tree']['topic'])\n",
    "    documents.append(tweet['interaction']['tag_tree']['topic'])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nTopicError+=1\n",
    "  try:\n",
    "    outList.append(tweet['interaction']['tag_tree']['topic'].values()[0])\n",
    "    documents.append(tweet['interaction']['tag_tree']['topic'].values()[0])\n",
    "  except:\n",
    "    outList.append('NaN')\n",
    "    nSubTopicError+=1\n",
    "\n",
    "  outFile.writerow(outList)\n",
    "\n",
    "print \"%d ID errors.\" % nIdError\n",
    "print \"%d Date errors.\" % nDateError\n",
    "print \"%d Content errors.\" % nContentError\n",
    "print \"%d Type errors.\" % nTypeError\n",
    "print \"%d DataSift language errors.\" % nLanguageError\n",
    "print \"%d Twitter language errors.\" % nTwitterLanguageError\n",
    "print \"%d Twitter Location errors.\" % nLocationError\n",
    "print \"%d Twitter Latitude errors.\" % nLatError\n",
    "print \"%d Twitter Longitude errors.\" % nLongError\n",
    "print \"%d UNGP Location errors.\" % nUngpLocationError\n",
    "print \"%d Gender errors.\" % nGenderError\n",
    "print \"%d UNGP gender errors.\" % nUngpGenderError\n",
    "print \"%d UNGP gender probability errors.\" % nUngpGenderProbError\n",
    "print \"%d Follower errors.\" % nFollowersError\n",
    "print \"%d Friends errors.\" % nFriendsError\n",
    "print \"%d Sentiment errors.\" % nSentimentError\n",
    "print \"%d Topic errors.\" % nTopicError\n",
    "print \"%d Sub-topic errors.\" % nSubTopicError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url(https://fonts.googleapis.com/css?family=Roboto+Slab:400,100);\n",
       "@import url(https://fonts.googleapis.com/css?family=Roboto+Mono:300);\n",
       "\n",
       "body{\n",
       "  font-family: \"Helvetica\", \"Helvetica Neue\", sans-serif;\n",
       "}\n",
       "\n",
       "#notebook {\n",
       "  font-size: 1.1em !important;\n",
       "  font-weight: 100;\n",
       "}\n",
       "\n",
       "a {\n",
       "\tcolor: #138331;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5 {\n",
       "  font-family: \"Roboto Slab\", sans-serif;\n",
       "  font-weight: 100 !important;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "  font-family: \"Roboto Mono\", monospace;\n",
       "  font-weight: 300;\n",
       "}\n",
       "\n",
       "div.prompt{\n",
       "  font-family: \"Roboto Mono\", monospace;\n",
       "  font-weight: 300;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "styles = open(\"../css/custom.css\", \"r\").read()\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
